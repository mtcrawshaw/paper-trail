Paper Title,Abstract,Authors,Label,Date published,Date added,Date read,Link,Read,Recorded,Parent,
Building Machines that Learn and Think Like People,,"Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman",Deep learning commentary,,5/20/2019,5/1/2019,https://arxiv.org/pdf/1604.00289.pdf,TRUE,FALSE,None,
Learning to Navigate in Complex Environments,,"Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell",Navigation,,5/20/2019,5/20/2019,https://arxiv.org/pdf/1611.03673.pdf,TRUE,TRUE,None,
Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification,,"Yuting Zhang, Kibok Lee, Honglak Lee",NN training methods,,5/20/2019,,https://arxiv.org/pdf/1606.06582,FALSE,FALSE,Learning to Navigate in Complex Environments,
Deep Successor Reinforcement Learning,,"Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman",Reinforcement learning,,5/20/2019,,https://arxiv.org/pdf/1606.02396,FALSE,FALSE,Learning to Navigate in Complex Environments,
A Deep Hierarchical Approach to Lifelong Learning in Minecraft,,"Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, Shie Mannor",Reinforcement learning,,5/20/2019,,https://arxiv.org/pdf/1604.07255,FALSE,FALSE,Learning to Navigate in Complex Environments,
Context encoders: Feature Learning by Inpainting,,"Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros",Computer vision,,5/21/2019,,https://arxiv.org/pdf/1604.07379,FALSE,FALSE,Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks,
Human-level concept learning through probabilistic program induction,,"Brenden M. Lake, Ruslan Salakhutdinov, Joshua B. Tenenbaum",Intelligence modeling,,5/21/2019,5/21/2019,https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf,TRUE,TRUE,Building Machines that Learn and Think Like People,
Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks,,"Dinesh Jayaraman, Kristen Grauman",Navigation,,5/21/2019,5/21/2019,https://arxiv.org/pdf/1709.00507.pdf,TRUE,TRUE,None,
Recurrent Models of Visual Attention,,"Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu",Computer vision,,5/22/2019,5/23/2019,https://arxiv.org/pdf/1406.6247.pdf,TRUE,FALSE,Building Machines that Learn and Think Like People,
Generative Temporal Models with Memory,,"Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir Mohamed, Danilo J. Rezende, David Amos, Timothy Lillicrap",Generative models,,5/23/2019,,https://arxiv.org/pdf/1702.04649,FALSE,FALSE,World Models,
Replay Comes of Age,,David J. Foster,Neuroscience,,5/23/2019,,https://www.annualreviews.org/doi/10.1146/annurev-neuro-072116-031538,FALSE,FALSE,World Models,
Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning,,R. J. Williams,Reinforcement learning,,5/23/2019,,http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf,FALSE,FALSE,Recurrent Models of Visual Attention,
World Models,,"David Ha, Jurgen Schmidhuber",Reinforcement learning,,5/23/2019,,https://arxiv.org/pdf/1803.10122.pdf,TRUE,FALSE,None,
Curiosity-driven Exploration by Self-supervised Prediction,,"Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell",Reinforcement learning,,5/23/2019,,https://arxiv.org/pdf/1705.05363,FALSE,FALSE,World Models,
On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models,,Jurgen Schmidhuber,Reinforcement learning,,5/23/2019,,https://arxiv.org/pdf/1511.09249,FALSE,FALSE,World Models,
Visualizing and Understanding Convolutional Networks,,"Mattew D. Zeiler, Rob Fergus",Computer vision,,5/27/2019,5/27/2019,https://arxiv.org/pdf/1311.2901.pdf,TRUE,FALSE,Building Machines that Learn and Think Like People,
Deep Visual Foresight for Planning Robot Motion,,"Chelsea Finn, Sergey Levine",Embodied agent,,5/27/2019,5/28/2019,https://arxiv.org/pdf/1610.00696.pdf,TRUE,TRUE,None,
Unsupervised Learning for Physical Interaction through Video Prediction,,"Chelsea Finn, Ian Goodfellow, Sergey Levin",Embodied agent,,5/27/2019,,,FALSE,FALSE,Deep Visual Foresight for Planning Robot Motion,
Learning to Poke by Poking: Experiential Learning of Intuitive Physics,,"Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, Sergey Levine",Embodied agent,,5/27/2019,,,FALSE,FALSE,Deep Visual Foresight for Planning Robot Motion,
Deep multi-scale video prediction beyond mean square error,,"Michael Mathieu, Camille Couprie, Yann LeCun",Generative models,,5/28/2019,,,FALSE,FALSE,Deep Visual Foresight for Planning Robot Motion,
Measuring the tendency of CNNs to Learn Surface Statistical Regularities,,"Jason Jo, Yoshua Bengio",Computer vision,,5/30/2019,06/01/2019,https://arxiv.org/pdf/1711.11561.pdf,TRUE,FALSE,None,
Explaining and Harnessing Adversarial Examples,,"Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy",Adversarial examples,,6/7/2019,,https://arxiv.org/pdf/1412.6572.pdf,FALSE,FALSE,Measuring the tendency of CNNs to Learn Surface Statistical Regularities,
Universal Adversarial Perturbations,,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard",Adversarial examples,,6/7/2019,,https://arxiv.org/pdf/1610.08401.pdf,FALSE,FALSE,Measuring the tendency of CNNs to Learn Surface Statistical Regularities,
Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks,,"Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami",Adversarial examples,,6/7/2019,,https://arxiv.org/pdf/1511.04508,FALSE,FALSE,Measuring the tendency of CNNs to Learn Surface Statistical Regularities,
Parseval Networks: Improving Robustness to Adversarial Examples,,"Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, Nicolas Usunier",Adversarial examples,,6/7/2019,,https://arxiv.org/pdf/1704.08847.pdf,FALSE,FALSE,Measuring the tendency of CNNs to Learn Surface Statistical Regularities,
"Formal theory of Creativity, Fun, and Intrinsic Motivation",,Jurgen Schmidhuber,Intelligence modeling,,6/7/2019,,http://people.idsia.ch/~juergen/ieeecreative.pdf,FALSE,FALSE,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,
Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,,"Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum",Reinforcement learning,,6/7/2019,,https://arxiv.org/pdf/1604.06057.pdf,FALSE,FALSE,Building Machines that Learn and Think Like People,
Core Knowledge,,Elizabeth S. Spelke and Katherine D. Kinzler,Neuroscience,,6/27/2019,,http://dsclab.uchicago.edu/documents/Spelke%20Kinzler%20Dev%20Sci%202007.pdf,FALSE,FALSE,Building Machines that Learn and Think Like People,
Design Principles of the Hippocampal Cognitive Map,,"Kimberly L. Stachenfeld, Matthew M. Botvinick, and Samuel J. Gershman",Neuroscience,,6/27/2019,,https://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map.pdf,FALSE,FALSE,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,
The Successor Representation and Temporal Context,,"Samuel J. Gershman, Christopher D. Moore, Michael T. Todd, Kenneth A. Norman, Per B. Sederberg",Neuroscience,,6/27/2019,,http://gershmanlab.webfactional.com/pubs/Gershman12.pdf,FALSE,FALSE,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,
Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective,,"Matthew M. Botvinick, Yael Niv, and Andrew C. Barto",Neuroscience,,6/27/2019,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2783353/,FALSE,FALSE,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,
Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning,,"Shakir Mohamed, Danilo Jimenez Rezende",Reinforcement learning,,6/27/2019,,https://arxiv.org/pdf/1509.08731,FALSE,FALSE,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,
Prioritized Experience Replay,,"Tom Schaul, John Quan, Ioannis Antonoglou, David Silver",Reinforcement learning,,6/27/2019,8/20/2019,https://arxiv.org/pdf/1511.05952,TRUE,TRUE,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,
Deep Exploration via Bootstrapped DQN,,"Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy",Reinforcement learning,,6/27/2019,,https://arxiv.org/pdf/1602.04621,FALSE,FALSE,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,
Xception: Deep Learning with Depthwise Separable Convolutions,,Francois Chollet,Computer vision,,7/12/2019,7/22/2019,https://arxiv.org/pdf/1610.02357,TRUE,TRUE,sharpDARTS: Faster and More Accurate Differentiable Architecture Search,
Exploring the Limits of Weakly Supervised Pretraining,,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten",Computer vision,,7/12/2019,,https://arxiv.org/pdf/1805.00932.pdf,FALSE,FALSE,None,
GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,,"Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Zhifeng Chen",Computer vision,,7/12/2019,,https://arxiv.org/pdf/1811.06965,FALSE,FALSE,None,Notes
sharpDARTS: Faster and More Accurate Differentiable Architecture Search,,"Andrew Hundt, Varun Jain, Gregory D. Hager",Neural architecture search,,7/12/2019,,https://arxiv.org/pdf/1903.09900.pdf,FALSE,FALSE,None,
MINE: Mutual Information Neural Estimation,,"Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, R Devon Hjelm",Information theory,,7/16/2019,,https://arxiv.org/pdf/1801.04062,FALSE,FALSE,Unsupervised State Representation Learning in Atari,
Predictive Information in a Sensory Population,,"Stephanie E. Palmer, Olivier Marre, Michael J. Berry II, William Bialek",Neuroscience,,7/16/2019,,https://arxiv.org/pdf/1307.0225,FALSE,FALSE,Unsupervised State Representation Learning in Atari,
Proximal Policy Optimization Algorithms,,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",Reinforcement learning,,7/16/2019,,https://arxiv.org/pdf/1707.06347,FALSE,FALSE,Unsupervised State Representation Learning in Atari,
Unsupervised State Representation Learning in Atari,,"Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, R. Devon Hjelm",Representation learning,,7/16/2019,7/18/2019,https://arxiv.org/pdf/1906.08226v2.pdf,TRUE,FALSE,None,
Representation Learning with Contrastive Predictive Coding,,"Aaron van den Oord, Yazhe Li, Oriol Vinyals",Representation learning,,7/16/2019,,https://arxiv.org/pdf/1807.03748,FALSE,FALSE,Unsupervised State Representation Learning in Atari,
Learning Deep Representations by Mutual Information Estimation and Maximization,,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, Yoshua Bengio",Representation learning,,7/16/2019,,https://arxiv.org/pdf/1808.06670,FALSE,FALSE,Unsupervised State Representation Learning in Atari,
MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,,"Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam",Computer vision,,7/23/2019,7/23/2019,https://arxiv.org/pdf/1704.04861,TRUE,TRUE,None,
Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,,"Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas",Multi agent reinforcement learning,,8/6/2019,8/6/2019,https://arxiv.org/pdf/1810.08647,TRUE,FALSE,None,
How Evolution May Work Through Curiosity-Driven Developmental Process,,"P. Y. Oudeyer, L. B. Smith",Developmental Psychology,,8/7/2019,,https://www.ncbi.nlm.nih.gov/pubmed/26969919,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input,,"Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, Stephen Clark",Emergent intelligence,,8/7/2019,,https://arxiv.org/pdf/1804.03984,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Emergent Communication through Negotiation,,"Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark",Emergent intelligence,,8/7/2019,,https://arxiv.org/pdf/1804.03980,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Emergence of Communication in an Interactive World with Consistent Speakers,,"Ben Bogin, Mor Geva, Jonathan Berant",Emergent intelligence,,8/7/2019,8/19/2019,https://arxiv.org/pdf/1809.00549,TRUE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Inequity aversion improves cooperation in intertemporal social dilemmas,,"Edward Hughes, Joel Z. Leibo, Matthew G. Phillips, Karl Tuyls, Edgar A. Duéñez-Guzmán, Antonio García Castañeda, Iain Dunning, Tina Zhu, Kevin R. McKee, Raphael Koster, Heather Roff, Thore Graepel",Multi agent reinforcement learning,,8/7/2019,,https://arxiv.org/pdf/1803.08884,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Consequentialist conditional cooperation in social dilemmas with imperfect information,,"Alexander Peysakhovich, Adam Lerer",Multi agent reinforcement learning,,8/7/2019,,https://arxiv.org/pdf/1710.06975,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Multi-agent Reinforcement Learning in Sequential Social Dilemmas,,"Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, Thore Graepel",Multi agent reinforcement learning,,8/7/2019,,https://arxiv.org/pdf/1702.03037,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch",Multi agent reinforcement learning,,8/7/2019,8/22/2019,https://arxiv.org/pdf/1706.02275,TRUE,TRUE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Optimal Rewards for Cooperative Agents,,"Bingyao Liu, Satinder Singh, Richard L. Lewis, Shiyin Qin",Multi agent reinforcement learning,,8/7/2019,,https://ieeexplore.ieee.org/document/6920028,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Machine Theory of Mind,,"Neil C. Rabinowitz, Frank Perbet, H. Francis Song, Chiyuan Zhang, S.M. Ali Eslami, Matthew Botvinick",Multi agent reinforcement learning,,8/7/2019,,https://arxiv.org/pdf/1802.07740,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Measuring collaborative emergent behavior in multi-agent reinforcement learning,,"Sean L. Barton, Nicholas R. Waytowich, Erin Zaroukian, Derrik E. Asher",Multi agent reinforcement learning,,8/7/2019,8/21/2019,https://arxiv.org/pdf/1807.08663,TRUE,TRUE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Compositional Obverter Communication Learning From Raw Visual Input,,"Edward Choi, Angeliki Lazaridou, Nando de Freitas",Reinforcement learning,,8/7/2019,,https://arxiv.org/pdf/1804.02341,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Learning to Share and Hide Intentions using Information Regularization,,"DJ Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, David Schwab",Reinforcement learning,,8/7/2019,,https://arxiv.org/pdf/1808.02093,FALSE,FALSE,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,
Weight Uncertainty in Neural Networks,,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra",Bayesian neural networks,,8/12/2019,,https://arxiv.org/pdf/1505.05424,FALSE,FALSE,None,
Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog,,"Satwik Kottur, José M.F. Moura, Stefan Lee, Dhruv Batra",Emergent intelligence,,8/19/2019,8/30/2019,https://arxiv.org/pdf/1706.08502,TRUE,TRUE,Emergence of Communication in an Interactive World with Consistent Speakers,
The HSIC Bottleneck: Deep Learning without Back-Propagation,,"Wan-Duo Kurt Ma, J.P. Lewis, W. Bastiaan Kleijn",NN training methods,,8/19/2019,,https://arxiv.org/pdf/1908.01580,FALSE,FALSE,None,
The Evolution of Language,,"Martin Nowak, Krakauer DC",Linguistics,,8/20/2019,,https://www.ncbi.nlm.nih.gov/pubmed/10393942,FALSE,FALSE,Emergence of Communication in an Interactive World with Consistent Speakers,
The Role of Hippocampal Replay in Memory and Planning,,"H. Freyja Ólafsdóttir, Daniel Bush, Caswell Barry",Neuroscience,,8/20/2019,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5847173/,FALSE,FALSE,Prioritized Experience Replay,
Deep Reinforcement Learning with Double Q-Learning,,"Hado van Hasselt, Arthur Guez, David Silver",Reinforcement learning,,8/20/2019,,https://arxiv.org/pdf/1509.06461,FALSE,FALSE,Prioritized Experience Replay,
Counterfactual Multi-Agent Policy Gradients,,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, Shimon Whiteson",Multi agent reinforcement learning,,8/21/2019,,https://arxiv.org/pdf/1705.08926,FALSE,FALSE,Measuring collaborative emergent behavior in multi-agent reinforcement learning,
Emergence of Grounded Compositional Language in Multi-Agent Populations,,"Igor Mordatch, Pieter Abbeel",Emergent intelligence,,8/22/2019,,https://arxiv.org/pdf/1703.04908,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Learning to Protect Communications with Adversarial Neural Cryptography,,"Martín Abadi, David G. Andersen",Emergent intelligence,,8/22/2019,,https://arxiv.org/pdf/1610.06918,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Predicting Pragmatic Reasoning in Language Games,,"Michael C. Frank, Noah D. Goodman",Linguistics,,8/22/2019,,https://science.sciencemag.org/content/336/6084/998,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Learning to Communicate with Deep Multi-Agent Reinforcement Learning,,"Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson",Multi agent reinforcement learning,,8/22/2019,8/29/2019,https://arxiv.org/pdf/1605.06676,TRUE,TRUE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Learning Multiagent Communication with Backpropagation,,"Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus",Multi agent reinforcement learning,,8/22/2019,,https://arxiv.org/pdf/1605.07736,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability,,"Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P. How, John Vian",Multi agent reinforcement learning,,8/22/2019,,https://arxiv.org/pdf/1703.06182,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play,,"Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, Rob Fergus",Reinforcement learning,,8/22/2019,,https://arxiv.org/pdf/1703.05407,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Policy Gradient Methods for Reinforcement Learning with Function Approximation,,"Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour",Reinforcement learning,,8/22/2019,,https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Emergent Complexity via Multi-Agent Competition,,"Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, Igor Mordatch",Emergent intelligence,,8/23/2019,,https://arxiv.org/pdf/1710.03748,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Safe and Nested Subgame Solving for Imperfect-Information Games,,"Noam Brown, Tuomas Sandholm",Game theory,,8/23/2019,,https://arxiv.org/pdf/1705.02955,FALSE,FALSE,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,
Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments,,"Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, Pieter Abbeel",Meta learning,,8/23/2019,,https://arxiv.org/pdf/1710.03641,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,,"Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel",Multi agent reinforcement learning,,8/23/2019,,https://arxiv.org/pdf/1711.00832,FALSE,FALSE,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
Iterated Learning and the Evolution of Language,,"S. Kirby, T. Griffiths, K. Smith",Evolutionary linguistics,,8/28/2019,,https://www.research.ed.ac.uk/portal/en/publications/iterated-learning-and-the-evolution-of-language(09e19225-6133-4dcf-b6ad-aba22651244c).html,FALSE,FALSE,Emergence of Grounded Compositional Language in Multi-Agent Populations,
The Evolution of Syntactic Communication,,"Martin Nowak, J. B. Plotkin, V. A. Jansen",Evolutionary linguistics,,8/28/2019,,https://www.ncbi.nlm.nih.gov/pubmed/10761917,FALSE,FALSE,Emergence of Grounded Compositional Language in Multi-Agent Populations,
Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,,Simon Kirby,Evolutionary linguistics,,8/28/2019,9/3/2019,https://link.springer.com/chapter/10.1007/3-540-48304-7_91,TRUE,TRUE,Emergence of Grounded Compositional Language in Multi-Agent Populations,
What Triggers the Emergence of Grammar?,,Luc Steels,Evolutionary linguistics,,8/28/2019,,http://digital.csic.es/handle/10261/128262,FALSE,FALSE,Emergence of Grounded Compositional Language in Multi-Agent Populations,
A Paradigm for Situated and Goal-Driven Language Learning,,"Jon Gauthier, Igor Mordatch",Natural language processing,,8/28/2019,,https://arxiv.org/abs/1610.03585,FALSE,FALSE,Emergence of Grounded Compositional Language in Multi-Agent Populations,
Modeling the Emergence of Language as an Embodied Collective Cognitive Activity,,"Edwin Hutchins, Christine M. Johnson",Computational cognitive science,,9/3/2019,,https://onlinelibrary.wiley.com/doi/full/10.1111/j.1756-8765.2009.01033.x,FALSE,FALSE,Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,
How Intrinsic Motivation Can Speed Up Language Emergence,,"Miquel Cornudella, Paul Van Eecke and Remi van Trijp",Emergent intelligence,,9/3/2019,,https://www.mitpressjournals.org/doi/pdf/10.1162/978-0-262-33027-5-ch100,FALSE,FALSE,Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,
Language Emergence in a Population of Artificial Agents Equipped with the Autotelic Principle,,"Miquel Cornudella, Thierry Poibeau",Emergent intelligence,,9/3/2019,,https://www.aclweb.org/anthology/W15-2407,FALSE,FALSE,Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,
Knowledge of Language,,Noam Chomsky,Evolutionary linguistics,,9/3/2019,,http://www.thatmarcusfamily.org/philosophy/Course_Websites/Readings/Chomsky%20-%20Knowledge%20of%20Language.pdf,FALSE,FALSE,Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,
Natural Language and Natural Selection,,"Steven Pinker, Paul Bloom",Evolutionary linguistics,,9/3/2019,,https://pdfs.semanticscholar.org/dbe7/9abbbc1fc7df6ce90d263a363d99fabd3489.pdf,FALSE,FALSE,Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,
"Learning, Bottlenecks, and the Evolution of Recursive Syntax",,Simon Kirby,Evolutionary linguistics,,9/3/2019,,"http://www.lel.ed.ac.uk/~simon/Papers/Kirby/Learning,%20Bottlenecks%20and%20the%20Evolution%20of%20Recursive%20Syntax.pdf",FALSE,FALSE,Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,Chunking algorithm for learning grammars from examples described here.
Minimal Requirements for the Cultural Evolution of Language,,Matthew Spike,Evolutionary linguistics,,9/3/2019,,https://www.era.lib.ed.ac.uk/handle/1842/25930,FALSE,FALSE,Syntax out of Learning: The Cultural Evolution of Structured Communication in a Population of Induction Algorithms,
Panoptic Segmentation,,"Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár",Computer vision,,9/4/2019,9/4/2019,https://arxiv.org/abs/1801.00868,TRUE,FALSE,None,
Fast Online Object Tracking and Segmentation: A Unifying Approach,,"Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr",Computer vision,,9/6/2019,9/6/2019,https://arxiv.org/abs/1812.05050,TRUE,TRUE,None,
Video Object Segmentation by Learning Location-Sensitive Embeddings,,"Hai Ci, Chunyu Wang, Yizhou Wang",Video object segmentation,,9/6/2019,,http://openaccess.thecvf.com/content_ECCV_2018/papers/Hai_Ci_Video_Object_Segmentation_ECCV_2018_paper.pdf,FALSE,FALSE,Fast Online Object Tracking and Segmentation: A Unifying Approach,
CNN in MRF: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF,,"Linchao Bao, Baoyuan Wu, Wei Liu",Video object segmentation,,9/6/2019,,https://arxiv.org/abs/1803.09453,FALSE,FALSE,Fast Online Object Tracking and Segmentation: A Unifying Approach,
Efficient Video Object Segmentation via Network Modulation,,"Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Aggelos K. Katsaggelos",Video object segmentation,,9/6/2019,,https://arxiv.org/abs/1802.01218,FALSE,FALSE,Fast Online Object Tracking and Segmentation: A Unifying Approach,
End-to-end representation learning for Correlation Filter based tracking,,"Jack Valmadre, Luca Bertinetto, João F. Henriques, Andrea Vedaldi, Philip H. S. Torr",Visual tracking,,9/6/2019,,https://arxiv.org/abs/1704.06036,FALSE,FALSE,Fast Online Object Tracking and Segmentation: A Unifying Approach,
Fully-Convolutional Siamese Networks for Object Tracking,,"Luca Bertinetto, Jack Valmadre, João F. Henriques, Andrea Vedaldi, Philip H. S. Torr",Visual tracking,,9/6/2019,,https://arxiv.org/abs/1606.09549,FALSE,FALSE,Fast Online Object Tracking and Segmentation: A Unifying Approach,
High Performance Visual Tracking with Siamese Region Proposal Network,,"Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, Xiaolin Hu",Visual tracking,,9/6/2019,,http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf,FALSE,FALSE,Fast Online Object Tracking and Segmentation: A Unifying Approach,
Learning Dynamic Memory Networks for Object Tracking,,"Tianyu Yang, Antoni B. Chan",Visual tracking,,9/6/2019,,https://arxiv.org/abs/1803.07268,FALSE,FALSE,Fast Online Object Tracking and Segmentation: A Unifying Approach,
Graph Convolutional Tracking,,"Junyu Gao, Tianzhu Zhang, Changsheng Xu",Visual tracking,,9/6/2019,,http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.pdf,FALSE,FALSE,None,
Unsupervised Meta-Learning for Reinforcement Learning,,"Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine",Meta learning,,9/7/2019,,https://arxiv.org/abs/1806.04640,FALSE,FALSE,Evolved Policy Gradients,
Meta-Learning via Learned Loss,,"Yevgen Chebotar, Artem Molchanov, Sarah Bechtle, Ludovic Righetti, Franziska Meier, Gaurav Sukhatme",Meta learning,,9/7/2019,9/12/2019,https://arxiv.org/abs/1906.05374,TRUE,FALSE,Evolved Policy Gradients,
Godel Machines: Fully Self-Referential Optimal Universal Self-Improvers,,Jurgen Schmidhuber,Meta learning,,9/7/2019,,ftp://ftp.idsia.ch/pub/juergen/gmAGI.pdf,FALSE,FALSE,Evolved Policy Gradients,
Meta-Learning and Universality: Deep Representations and Gradient Descent Can Approximate Any Learning Algorithm,,"Chelsea Finn, Sergey Levine",Meta learning,,9/7/2019,,https://arxiv.org/abs/1710.11622,FALSE,FALSE,Evolved Policy Gradients,
Evolved Policy Gradients,,"Rein Houthooft, Richard Y. Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho, Pieter Abbeel",Multi agent reinforcement learning,,9/7/2019,9/9/2019,https://arxiv.org/abs/1802.04821,TRUE,FALSE,None,
Evolving Intrinsic Motivations for Altruistic Behavior,,"Jane Wang, Edward Hughes, Chrisantha Fernando, Wojciech Czarnecki, Edgar Duenez-Guzman, Joel Leibo",Multi agent reinforcement learning,,9/7/2019,9/10/2019,https://dl.acm.org/citation.cfm?id=3331756,TRUE,TRUE,Evolved Policy Gradients,
Genetic Programming for Reward Function Search,,"Scott Niekum, Andrew Barto",Reinforcement learning,,9/7/2019,,https://www.ece.uvic.ca/~bctill/papers/ememcog/Niekum_etal_2010.pdf,FALSE,FALSE,Evolved Policy Gradients,
Evolution of Reward Functions for Reinforcement Learning,,"Scott Niekum, Lee Spector, Andrew Barto",Reinforcement learning,,9/7/2019,,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.479.4791&rep=rep1&type=pdf,FALSE,FALSE,Evolved Policy Gradients,
Intrinsically Motivated Reinforcement Learning: An Evolutionary Perspective,,"Satinder Singh, Richard Lewis, Andrew Barto",Reinforcement learning,,9/7/2019,,https://web.eecs.umich.edu/~baveja/Papers/IMRLIEEETAMDFinal.pdf,FALSE,FALSE,Evolved Policy Gradients,
Active Reward Learning with a Novel Acquisition Function,,"Christian Daniel, Olivia Kroemer, Malte Viering, Jan Metz, Jan Peters",Reinforcement learning,,9/7/2019,,https://dl.acm.org/citation.cfm?id=2825774,FALSE,FALSE,None,
"Reinforcement Learning, Fast and Slow",,"Mathew Botvinick, Jane Wang, Sam Ritter, Zeb Kurth-Nelson, Charles Blundell, Demis Hassabis",Reinforcement learning,,9/7/2019,9/11/2019,https://www.sciencedirect.com/science/article/pii/S1364661319300610,TRUE,TRUE,Evolving Intrinsic Motivations for Altruistic Behavior,
Reinforcement Learning with Unsupervised Auxiliary Tasks,,"Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu",Reinforcement learning,,9/7/2019,,https://arxiv.org/pdf/1611.05397.pdf,FALSE,FALSE,None,
Evolution Strategies as a Scalable Alternative to Reinforcement Learning,,"Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, Ilya Sutskever",Evolution strategies,,9/9/2019,,https://arxiv.org/abs/1703.03864,FALSE,FALSE,Evolved Policy Gradients,
Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,,"Chelsea Finn, Pieter Abbeel, Sergey Levine",Meta learning,,9/9/2019,,https://arxiv.org/abs/1702.08165,FALSE,FALSE,Evolved Policy Gradients,
RL2: Fast Reinforcement Learning via Slow Reinforcement Learning,,"Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, Pieter Abbeel",Meta learning,,9/9/2019,,https://arxiv.org/abs/1611.02779,FALSE,FALSE,Evolved Policy Gradients,
Random Gradient-Free Minimization of Convex Functions,,"Yurii Nesterov, Vladimir Spokoiny",Optimization,,9/9/2019,,https://link.springer.com/article/10.1007/s10208-015-9296-2,FALSE,FALSE,Evolved Policy Gradients,
Trust Region Policy Optimization,,"John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel",Reinforcement learning,,9/9/2019,,https://arxiv.org/abs/1502.05477,FALSE,FALSE,Evolved Policy Gradients,
Count-Based Exploration with Neural Density Models,,"Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, Remi Munos",Reinforcement learning,,9/9/2019,,https://arxiv.org/abs/1703.01310,FALSE,FALSE,Evolved Policy Gradients,
Bridging the Gap Between Value and Policy Based Reinforcement Learning,,"Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans",Reinforcement learning,,9/9/2019,,https://arxiv.org/abs/1702.08892,FALSE,FALSE,Evolved Policy Gradients,
Reinforcement Learning with Deep Energy-Based Policies,,"Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine",Reinforcement learning,,9/9/2019,,https://arxiv.org/abs/1702.08165,FALSE,FALSE,Evolved Policy Gradients,
Gradient Estimation using Stochastic Computation Graphs,,Gradient Estimation Using Stochastic Computation Graphs,Reinforcement learning,,9/9/2019,,https://arxiv.org/abs/1506.05254,FALSE,FALSE,Evolved Policy Gradients,
Reward Design via Online Gradient Ascent,,"Jonathon Sorg, Richard Lewis, Satinder Singh",Reinforcement learning,,9/9/2019,,https://papers.nips.cc/paper/4146-reward-design-via-online-gradient-ascent,FALSE,FALSE,Evolved Policy Gradients,
Exploring the Predictable,,Jurgen Schmidhuber,Reinforcement learning,,9/9/2019,,https://link.springer.com/chapter/10.1007/978-3-642-18965-4_23,FALSE,FALSE,Evolved Policy Gradients,
Horizontal Gene Transfer and the Evolution of Bacterial Cooperation,,"Sorcha McGinty, Daniel Rankin, Sam Brown",Evolutionary biology,,9/10/2019,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3038327/,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
A Theory of Group Selection,,David Wilson,Evolutionary theory,,9/10/2019,,https://www.pnas.org/content/72/1/143,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Five Rules for the Evolution of Cooperation,,Martin Nowak,Evolutionary theory,,9/10/2019,,https://science.sciencemag.org/content/314/5805/1560,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Greenbeards,,"Andy Gardner, Stuart West",Evolutionary theory,,9/10/2019,,https://onlinelibrary.wiley.com/doi/full/10.1111/j.1558-5646.2009.00842.x,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
"Cultural group selection, coevolutionary processes and large-scale cooperation",,Joseph Henrich,Evolutionary theory,,9/10/2019,,https://www.sciencedirect.com/science/article/pii/S0167268103000945,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Kin Selection: Fact and Fiction,,"Ashleigh Griffin, Stuart West",Evolutionary theory,,9/10/2019,,https://www.sciencedirect.com/science/article/pii/S0169534701023552,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Meta-Learning by the Baldwin Effect,,"Chrisantha Thomas Fernando, Jakub Sygnowski, Simon Osindero, Jane Wang, Tom Schaul, Denis Teplyashin, Pablo Sprechmann, Alexander Pritzel, Andrei A. Rusu",Meta learning,,9/10/2019,,https://arxiv.org/abs/1806.07917,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
A multi-agent reinforcement learning model of common-pool resource appropriation,,"Julien Perolat, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, Thore Graepel",Multi agent reinforcement learning,,9/10/2019,,https://arxiv.org/abs/1707.06600,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Learning with Opponent-Learning Awareness,,"Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, Igor Mordatch",Multi agent reinforcement learning,,9/10/2019,,https://arxiv.org/abs/1709.04326,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Cooperative Coevolution of Multi-Agent Systems,,"Chern Yong, Risto Miikkulainen",Multi agent reinforcement learning,,9/10/2019,,http://nn.cs.utexas.edu/downloads/papers/yong.tr287.pdf,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Population Based Training of Neural Networks,,"Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu",NN training methods,,9/10/2019,,https://arxiv.org/abs/1711.09846,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu",Reinforcement learning,,9/10/2019,,https://arxiv.org/abs/1802.01561,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Where Do Rewards Come From?,,"Satinder Singh, Richard Lewis, Andrew Barto",Reinforcement learning,,9/10/2019,9/16/2019,http://www-anw.cs.umass.edu/legacy/pubs/2009/singh_l_b_09.pdf,TRUE,TRUE,Evolving Intrinsic Motivations for Altruistic Behavior,
Nice Guys Finish First: The Competitive Altruism Hypothesis,,"Charlie Hardy, Mark Van Vugt",Social psychology,,9/10/2019,,https://journals.sagepub.com/doi/abs/10.1177/0146167206291006,FALSE,FALSE,Evolving Intrinsic Motivations for Altruistic Behavior,
Deep Learning: A Critcal Appraisal,,Gary Marcus,Deep learning commentary,,9/11/2019,,https://arxiv.org/abs/1801.00631,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
How Learning Can Guide Evolution,,"Geoffrey Hinton, Steven Nowlan",Evolutionary theory,,9/11/2019,,https://pdfs.semanticscholar.org/5d6c/84e7cd46d0a520ad6784a0f7f6825ef83685.pdf,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
"Been There, Done That: Meta-Learning with Episodic Recall",,"Samuel Ritter, Jane X. Wang, Zeb Kurth-Nelson, Siddhant M. Jayakumar, Charles Blundell, Razvan Pascanu, Matthew Botvinick",Meta learning,,9/11/2019,,https://arxiv.org/abs/1805.09692,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Toward an Integration of Deep Learning and Neuroscience,,"Adam Marblestone, Greg Wayne, Konrad Kording",Neuroscience,,9/11/2019,,https://arxiv.org/abs/1606.03813,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Reinforcement Learning and Episode Memory in Humans and Animals: An Integrative Framework,,"Samuel Gershman, Nathaniel Daw",Neuroscience,,9/11/2019,,http://gershmanlab.webfactional.com/pubs/GershmanDaw17.pdf,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
What is a Cognitive Map? Organizing Knowledge for Flexible Behavior,,"Timothy Behrens, Timothy Muller, James Whittington, Shirley Mark,  Alon Baram, Kimberly Stachenfeld, ZebKurth-Nelson",Neuroscience,,9/11/2019,,https://www.sciencedirect.com/science/article/abs/pii/S0896627318308560,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Hippocampal Pattern Separation Supports Reinforcement Learning,,"Ian Ballard, Anthony Wagner, Samuel McClure",Neuroscience,,9/11/2019,,https://www.ncbi.nlm.nih.gov/pubmed/30842581,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.,,"James McClelland, Bruce McNaughton, Randall O'Reilly",Neuroscience,,9/11/2019,,https://www.ncbi.nlm.nih.gov/pubmed/7624455,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Goals and Habits in the Brain,,"Ray Dolan, Peter Dayan",Neuroscience,,9/11/2019,,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3807793/,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Prefrontal cortex as a meta-reinforcement learning system,,"Jane X. Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Demis Hassabis & Matthew Botvinick","Neuroscience, meta learning",,9/11/2019,,https://www.ncbi.nlm.nih.gov/pubmed/29760527,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Neural Circuitry of Reward Prediction Error,,"Mitsuko Watabe-Uchida, Neir Eshel, Naoshige Uchida","Neuroscience, reinforcement learning",,9/11/2019,,https://www.ncbi.nlm.nih.gov/pubmed/28441114,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
"Train Faster, Generalize Better: Stability of Stochastic Gradient Descent",,"Moritz Hardt, Benjamin Recht, Yoram Singer",Optimization,,9/11/2019,,https://arxiv.org/abs/1509.01240,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Neural Episodic Control,,"Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adrià Puigdomènech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell",Reinforcement learning,,9/11/2019,,https://arxiv.org/abs/1703.01988,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Unsupervised Predictive Memory in a Goal Directed Agent,,"Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska-Barwinska, Jack Rae, Piotr Mirowski, Joel Z. Leibo, Adam Santoro, Mevlana Gemici, Malcolm Reynolds, Tim Harley, Josh Abramson, Shakir Mohamed, Danilo Rezende, David Saxton, Adam Cain, Chloe Hillier, David Silver, Koray Kavukcuoglu, Matt Botvinick, Demis Hassabis, Timothy Lillicrap","Reinforcement learning, meta learning",,9/11/2019,,https://arxiv.org/abs/1803.10760,FALSE,FALSE,"Reinforcement Learning, Fast and Slow",
Unsupervised Learning via Meta Learning,,"Kyle Hsu, Sergey Levine, Chelsea Finn",Meta learning,,9/12/2019,,https://arxiv.org/abs/1810.02334,FALSE,FALSE,Meta Learning via Learned Loss,
Learning to learn by gradient descent by gradient descent,,"Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas",Meta learning,,9/12/2019,,https://arxiv.org/abs/1606.04474,FALSE,FALSE,Meta Learning via Learned Loss,
Reward Shaping via Meta-Learning,,"Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, Jun Zhu",Meta learning,,9/12/2019,,https://arxiv.org/abs/1901.09330,FALSE,FALSE,Meta Learning via Learned Loss,
Learning to Teach with Dynamic Loss Functions,,"Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Jianhuang Lai, Tie-Yan Liu",Meta learning,,9/12/2019,,https://arxiv.org/abs/1810.12081,FALSE,FALSE,Meta Learning via Learned Loss,
Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research,,"Joel Z. Leibo, Edward Hughes, Marc Lanctot, Thore Graepel","Emergent intelligence, multi agent reinforcement learning",,9/16/2019,,https://arxiv.org/abs/1903.00742,FALSE,FALSE,None,
Malthusian Reinforcement Learning,,"Joel Z. Leibo, Julien Perolat, Edward Hughes, Steven Wheelwright, Adam H. Marblestone, Edgar Duéñez-Guzmán, Peter Sunehag, Iain Dunning, Thore Graepel","Emergent intelligence, multi agent reinforcement learning",,9/16/2019,,https://arxiv.org/abs/1812.07019,FALSE,FALSE,None,
Neuronal Reward and Decision Signals: From Theories to Data,,Wolfram Schultz,"Neuroscience, reinforcement learning",,9/16/2019,,https://www.physiology.org/doi/full/10.1152/physrev.00023.2014,FALSE,FALSE,Where Do Rewards Come From?,